{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MD1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPdYr2ETZVEN0aXwB1C7h6W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TetianaHrunyk/NeuralMetaphorsDetection/blob/master/MD1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DN-fdELxpEH"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "from torch.utils.data import DataLoader\n",
        "import regex\n",
        "from time import time"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25fZrwqmnJ0N"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_errors(title, training_losses, test_loss=None, block=True):\n",
        "    plt.figure(num=title)\n",
        "    plt.plot(training_losses, label='Training loss')\n",
        "\n",
        "    if test_loss:\n",
        "        plt.plot([test_loss]*len(training_losses), label='Test loss')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.legend()\n",
        "    plt.show(block=block)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx03GOsa5rz1"
      },
      "source": [
        "# Load the data, create custom data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o60aeRIwx0xh"
      },
      "source": [
        "data_path='https://raw.githubusercontent.com/TetianaHrunyk/NeuralMetaphorsDetection/master/'"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMn3a3qgr0hx"
      },
      "source": [
        "for i in range(df.shape[0]):\n",
        "  if type(df.txt.iloc[i]) != str:\n",
        "    print(df.txt.iloc[i])"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKB9eJ9Rz2Pn"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, path, nrows=None, tokenizer=None):\n",
        "    self.text = pd.read_csv(path, header=None, names=[\"txt\"], dtype='str', nrows=nrows)\n",
        "    self.text.dropna(inplace=True)\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.text.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    tokenized = self.tokenizer(self.text.txt.iloc[idx])\n",
        "    labels = \" \".join([str(int(word.startswith(\"M_\"))) for word in tokenized])\n",
        "    txt = \" \".join([word.replace(\"M_\", \"\").lower() for word in tokenized])\n",
        "    return labels, txt\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAxhGAz0XRyc"
      },
      "source": [
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9l86utH-GhO"
      },
      "source": [
        "TAGS = [\"0\", \"1\"]\n",
        "train_data = CustomDataset(data_path+\"vuamc_train.csv\", nrows=None, tokenizer=en_tokenizer)\n",
        "test_data = CustomDataset(data_path+\"vuamc_test.csv\",  nrows=100, tokenizer=en_tokenizer)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhHvdPEc5vU-"
      },
      "source": [
        "# Build Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvXIL2QK5f0U"
      },
      "source": [
        "def build_vocab(data):\n",
        "  counter = Counter()\n",
        "  for row in data:\n",
        "      counter.update(row[1].split(\" \"))\n",
        "  return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'], min_freq=1)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDuPQaKB59iN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "319227bf-e84d-431a-f4fe-f685512cd3de"
      },
      "source": [
        "vocab = build_vocab(train_data)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-c417a2b6752c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-c2e638490d19>\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m       \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<bos>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<eos>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-d2a9cf8df975>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"M_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"M_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/data/utils.py\u001b[0m in \u001b[0;36m_spacy_tokenize\u001b[0;34m(x, spacy)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_spacy_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got float)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQaBZHu97HTt"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEahlvyw84K5"
      },
      "source": [
        "text_pipeline = lambda x: [vocab[token] for token in x.rstrip(\"\\n\").split(\" \")]\n",
        "label_pipeline = lambda x: [ int(l) for l in x.split(\" \")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwWI3-mE9VDm"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for labels, text in batch:\n",
        "        processed_text = text_pipeline(text)\n",
        "        text_tensor = torch.tensor(processed_text, dtype=torch.int64)\n",
        "        text_list.append(text_tensor)        \n",
        "        \n",
        "        processed_labels=label_pipeline(labels)\n",
        "        label_tensor = torch.tensor(processed_labels, dtype=torch.int64)\n",
        "        label_list.append(label_tensor)\n",
        "\n",
        "    label_list = torch.cat(label_list)\n",
        "    text_list = torch.cat(text_list)\n",
        "    \n",
        "    return label_list.to(device), text_list.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBKbDduu8IS4"
      },
      "source": [
        "train_dataloader = DataLoader(train_data, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxQMRs9cd1BW"
      },
      "source": [
        "# Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m-F6XkcBibf"
      },
      "source": [
        "class LSTMTagger_Embed(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger_Embed, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)                                                                \n",
        "\n",
        "        self.hidden2tag =  nn.Linear(hidden_dim, tagset_size) \n",
        "        self.hidden = self.init_hidden()         \n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(1, 1, self.hidden_dim),                                     \n",
        "                torch.zeros(1, 1, self.hidden_dim))\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds =  self.word_embeddings(sentence)                                                                                               \n",
        "        lstm_out, self.hidden =  self.lstm(embeds.view(len(sentence), 1, -1))               \n",
        "        tag_prediction = self.hidden2tag(lstm_out.view(len(sentence), -1))                     \n",
        "        tag_scores = F.log_softmax(tag_prediction, dim=1)                       \n",
        "        return tag_scores\n",
        "\n",
        "def train(model, n_epochs, dataloader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_start = time()\n",
        "        for i, (tags, sentence) in enumerate(dataloader):\n",
        "            model.zero_grad()\n",
        "            model.hidden = model.init_hidden()\n",
        "            \n",
        "            tag_scores = model(sentence)\n",
        "            loss = loss_function(tag_scores, tags)\n",
        "            train_losses.append(loss)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        epoch_end = time()\n",
        "        print('Epoch %d, loss: %0.4f, epoch took %d sec' % (epoch + 1, loss, round(epoch_end-epoch_start)))\n",
        "    \n",
        "    return train_losses\n",
        "\n",
        "def report_accuracy(model, dataloader, n=None, print_data=False):\n",
        "    if n==None:\n",
        "      n=TRAIN_DATA_LEN\n",
        "    with torch.no_grad():\n",
        "        total, total_correct, total_exact_correct = 0, 0, 0\n",
        "        TP, TN, FP, FN = 0, 0, 0, 0\n",
        "        for tags, sentence in dataloader:\n",
        "            scores = model(sentence)\n",
        "            out = torch.argmax(scores, dim=1).tolist()\n",
        "            targets = tags.tolist()\n",
        "            \n",
        "            correct = 0\n",
        "            length = len(targets)\n",
        "            \n",
        "            for i in range(length):\n",
        "                if out[i] == targets[i]:\n",
        "                  correct += 1\n",
        "                  if targets[i] == 1:\n",
        "                    TP += 1\n",
        "                  elif targets[i] == 0:\n",
        "                    TN += 1\n",
        "                else:\n",
        "                  if targets[i] == 1:\n",
        "                    FN += 1\n",
        "                  elif targets[i] == 0:\n",
        "                    FP += 1\n",
        "\n",
        "            if (print_data):\n",
        "                print('data: ' + str(dict(zip(sentence, tags))))\n",
        "                print('pred: ' + str(dict(zip(sentence, out))))\n",
        "                print('Correct: %d of %d' % (correct, length))\n",
        "\n",
        "            total += length\n",
        "            total_correct += correct\n",
        "            if correct == length:\n",
        "                total_exact_correct += 1\n",
        "\n",
        "        print('Accuracy (exact): %d / %d, %0.4f' % (total_exact_correct, n, total_exact_correct / n))\n",
        "        print('Accuracy: %d / %d, %0.4f \\n' % (total_correct, total, total_correct / total))\n",
        "        print(\"{:5}| {:10} | {:10}\\n{}\".format(\"\", \"T\", \"F\", \"-\"*30))\n",
        "        print(\"{:5}| {:10} | {:10}\".format(\"P\", TP, FP))\n",
        "        print(\"{:5}| {:10} | {:10}\\n\".format(\"N\", TN, FN))\n",
        "        \n",
        "        den = 0.00001 if TP+FP == 0 else TP+FP\n",
        "        print(\"Precision: \", round(TP/den, 2))\n",
        "        den = 0.00001 if TP+FN == 0 else TP+FN\n",
        "        print(\"Recall: \", round(TP/den, 2))\n",
        "        den = 0.00001 if TP+TN+FP+FN == 0 else TP+TN+FP+FN\n",
        "        print(\"F1: \", 2*TP/den)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8WRBygrVrWS"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv1NROFtVI7w"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "HIDDEN_DIM = 10  \n",
        "EMBEDDING_DIM = 64 \n",
        "n_epochs = 3      \n",
        "TRAIN_DATA_LEN = len(train_data)\n",
        "TEST_DATA_LEN = len(test_data)\n",
        "TAGS = [0, 1]\n",
        "\n",
        "\n",
        "model = LSTMTagger_Embed(EMBEDDING_DIM, HIDDEN_DIM, len(vocab), len(TAGS))\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)            \n",
        "loss_function = nn.NLLLoss()      \n",
        "\n",
        "train_losses = train(model, n_epochs, train_dataloader, loss_function, optimizer)\n",
        "\n",
        "print(\"\\nTraining Accuracy\")\n",
        "report_accuracy(model, train_dataloader)\n",
        "plot_errors(training_losses=train_losses, title='Training loss')\n",
        "#print(\"\\nAccuracy for testing data\")\n",
        "#report_accuracy(model, test_dataloader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx2qKLcCfirJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}